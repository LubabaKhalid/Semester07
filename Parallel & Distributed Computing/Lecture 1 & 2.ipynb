{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrT4te_xrxi0"
   },
   "source": [
    "# Lecture 1 & 2 (Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENn-D3CyWAZ3"
   },
   "source": [
    "# Real-World Scenario: Training a Neural Network\n",
    "## The Challenge:\n",
    "1. Imagine you need to train a modern language model with 175 billion parameters on a dataset of 100 million text samples.\n",
    "\n",
    "Single CPU (laptop): 45 days of continuous processing\n",
    "Distributed cluster (64 GPUs): 3-4 hours\n",
    "\n",
    "2. Processing Customer Data\n",
    "\n",
    "Single-threaded pandas: Processing 50GB customer transaction data = 8+ hours\n",
    "\n",
    "Parallel processing with Dask: Same task = 25 minutes\n",
    "\n",
    "## The Core Problem\n",
    "Moore's Law is ending. CPU clock speeds have matured around 3-4 GHz since 2005. Instead of faster cores, we now have:\n",
    "\n",
    "- More cores per CPU (8, 16, 32+ cores)\n",
    "- Specialized hardware (GPUs with thousands of cores)\n",
    "- Distributed systems (thousands of machines working together)  \n",
    "\n",
    "Key Insight: To handle growing data volumes and model complexity, we must embrace parallelism.\n",
    "\n",
    "## Why Parallel Computing Matters in Data Science & ML\n",
    "Modern Challenges in DS/ML\n",
    "1. Massive Dataset Sizes\n",
    "\n",
    "Traditional: MB to GB datasets\n",
    "\n",
    "Today: TB to PB datasets\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Netflix: 200+ billion events per day\n",
    "- Facebook: 4 petabytes of new data daily\n",
    "- Genomics: Single human genome = 200GB+ when fully analyzed\n",
    "\n",
    "\n",
    "\n",
    "2. Complex Model Architectures\n",
    "\n",
    "- 2012 AlexNet: 60M parameters\n",
    "- 2019 GPT-2: 1.5B parameters\n",
    "- 2023 GPT-4: Estimated 1.7T+ parameters\n",
    "Training time scales exponentially with model size\n",
    "\n",
    "3. Real-Time Requirements\n",
    "\n",
    "Recommendation systems: < 100ms response time\n",
    "\n",
    "High-frequency trading: < 1ms decisions\n",
    "\n",
    "Autonomous vehicles: < 10ms for safety-critical decisions\n",
    "\n",
    "## Benefits of Parallelism\n",
    "1. Speed Gains\n",
    "\n",
    "python# Sequential processing\n",
    "for batch in dataset:\n",
    "    model.train(batch)  # Takes 1 second per batch\n",
    "\n",
    "Total: 1000 batches × 1 sec = 1000 seconds\n",
    "\n",
    "Parallel processing (8 workers)\n",
    "\n",
    "parallel_workers = 8\n",
    "\n",
    "Total: 1000 batches ÷ 8 workers × 1 sec = 125 seconds\n",
    "\n",
    "Speedup: 8x faster!\n",
    "\n",
    "2. Scalability\n",
    "\n",
    "- Handle datasets larger than single machine memory\n",
    "- Train models too large for single GPU\n",
    "- Process streams of real-time data\n",
    "\n",
    "3. Hardware Utilization\n",
    "\n",
    "- CPU: Use all cores instead of just one\n",
    "- GPU: Leverage thousands of CUDA cores\n",
    "- Memory: Distribute data across multiple machines\n",
    "- Network: Pipeline data processing and transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCYXoCjse3XK"
   },
   "source": [
    "\n",
    "## Types of Parallelism\n",
    "1. Data Parallelism\n",
    "Concept: Same model/algorithm processes different portions of data simultaneously.\n",
    "\n",
    "How it works:\n",
    "Original Dataset: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "Split across 3 workers:\n",
    "- Worker 1: [1, 2, 3, 4]    → processes subset\n",
    "- Worker 2: [5, 6, 7]       → processes subset  \n",
    "- Worker 3: [8, 9, 10]      → processes subset\n",
    "\n",
    "Combine results: [result1, result2, result3]\n",
    "\n",
    "### Real Example - Neural Network Training:\n",
    "\n",
    "Each GPU gets different batches of the same data\n",
    "- GPU 0: processes batch [0:32]    using same model\n",
    "- GPU 1: processes batch [32:64]   using same model\n",
    "- GPU 2: processes batch [64:96]   using same model\n",
    "- GPU 3: processes batch [96:128]  using same model\n",
    "\n",
    "Gradients are averaged across all GPUs and Model parameters are synchronized\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- Training neural networks\n",
    "- Processing large datasets with same algorithm\n",
    "- Monte Carlo simulations\n",
    "- Image/video processing pipelines\n",
    "\n",
    "2. Task Parallelism\n",
    "\n",
    "Concept: Different tasks/functions run simultaneously on same or different data.\n",
    "\n",
    "Pipeline Example:\n",
    "\n",
    "Data Flow: Raw Data → Preprocess → Feature Extract → Model Train → Evaluate\n",
    "\n",
    "Sequential:\n",
    "[Load] → [Clean] → [Extract] → [Train] → [Eval]\n",
    "\n",
    "Total time: 5 + 3 + 4 + 10 + 2 = 24 minutes\n",
    "\n",
    "### Parallel Pipeline:\n",
    "- Time 0-5:   [Load Batch 1]\n",
    "- Time 3-8:   [Load Batch 2] + [Clean Batch 1]  \n",
    "- Time 6-11:  [Load Batch 3] + [Clean Batch 2] + [Extract Batch 1]\n",
    "- Time 9-19:  [Clean Batch 3] + [Extract Batch 2] + [Train Batch 1]\n",
    "\n",
    "...\n",
    "\n",
    "Total time: ~12 minutes (2x speedup)\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- ETL pipelines\n",
    "- Real-time data streaming\n",
    "- Multi-model ensemble training\n",
    "- Independent experiments running simultaneously\n",
    "\n",
    "3. Model Parallelism\n",
    "\n",
    "### Concept:\n",
    "Split a single large model across multiple devices.\n",
    "### Layer-wise Splitting:\n",
    "Large Neural Network:\n",
    "\n",
    "Insert image here\n",
    "\n",
    "### When to Use Model Parallelism:\n",
    "\n",
    "- Model too large for single GPU memory\n",
    "- Training very large language models (GPT-4, PaLM)\n",
    "- Memory-bound rather than compute-bound scenarios\n",
    "\n",
    "| Type            | Best For                              | Example                    | Complexity |\n",
    "|-----------------|---------------------------------------|----------------------------|------------|\n",
    "| **Data Parallel**  | Same computation on different data     | Training on multiple batches | Low        |\n",
    "| **Task Parallel**  | Different operations simultaneously   | ETL pipeline stages          | Medium     |\n",
    "| **Model Parallel** | Very large models                     | GPT-4 training               | High       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm6s9rWymRF6"
   },
   "source": [
    "# Python's Role in Parallel and Distributed Computing\n",
    "## Python's Strengths for Parallel Computing\n",
    "1. Rich Ecosystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iJF5AH30mdpG"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\PMLS\\%USERPROFILE%\\ray312\\Lib\\site-packages\\tensorflow\\python\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m    \u001b[38;5;66;03m# scikit-learn (traditional ML)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m            \u001b[38;5;66;03m# PyTorch (deep learning)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m \u001b[38;5;66;03m# TensorFlow (Google's ML framework)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Parallel / distributed computing\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m  \u001b[38;5;66;03m# Built-in parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\%USERPROFILE%\\ray312\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\PMLS\\%USERPROFILE%\\ray312\\Lib\\site-packages\\tensorflow\\python\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Scientific computing\n",
    "import numpy as np      # Optimized linear algebra\n",
    "import pandas as pd     # Data manipulation\n",
    "import scipy as sp      # Scientific algorithms\n",
    "\n",
    "# Machine learning\n",
    "import sklearn as sk    # scikit-learn (traditional ML)\n",
    "import torch            # PyTorch (deep learning)\n",
    "import tensorflow as tf # TensorFlow (Google's ML framework)\n",
    "\n",
    "# Parallel / distributed computing\n",
    "import multiprocessing as mp  # Built-in parallelism\n",
    "import dask                    # Dask (Pandas/NumPy scaling)\n",
    "import ray                     # Ray (distributed computing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
      "Executable: C:\\Users\\PMLS\\%USERPROFILE%\\ray312\\Scripts\\python.exe\n",
      "USERPROFILE: C:\\Users\\PMLS\n",
      "PYTHONPATH: None\n",
      "PIP_TARGET: None\n",
      "PYTHONUSERBASE: None\n",
      "['C:\\\\Users\\\\PMLS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip',\n",
      " 'C:\\\\Users\\\\PMLS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs',\n",
      " 'C:\\\\Users\\\\PMLS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib',\n",
      " 'C:\\\\Users\\\\PMLS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312',\n",
      " 'C:\\\\Users\\\\PMLS\\\\%USERPROFILE%\\\\ray312']\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pprint\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"USERPROFILE:\", os.environ.get(\"USERPROFILE\"))\n",
    "print(\"PYTHONPATH:\", os.environ.get(\"PYTHONPATH\"))\n",
    "print(\"PIP_TARGET:\", os.environ.get(\"PIP_TARGET\"))\n",
    "print(\"PYTHONUSERBASE:\", os.environ.get(\"PYTHONUSERBASE\"))\n",
    "pprint.pp(sys.path[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjomdSCFr9q9"
   },
   "source": [
    "### Easy integration with High Performance Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2DgzzZ4eZRJ"
   },
   "outputs": [],
   "source": [
    "# NumPy operations run in C\n",
    "import numpy as np\n",
    "large_array = np.random.random((10000, 10000))\n",
    "result1 = np.dot(large_array, large_array.T)  # C-speed matrix multiplication\n",
    "\n",
    "# CUDA operations through PyTorch\n",
    "import torch\n",
    "gpu_tensor = torch.randn(10000, 10000).cuda()\n",
    "result2 = torch.mm(gpu_tensor, gpu_tensor.T)  # GPU-accelerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYLZfSr-qe1x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goEk3E-k5Ig5"
   },
   "source": [
    "## Process Memory Model\n",
    "Each process has its own independent memory space, which includes:\n",
    "\n",
    "- Virtual address space: Completely isolated from other processes\n",
    "- Code segment: Program instructions\n",
    "- Data segment: Global and static variables\n",
    "- Heap: Dynamically allocated memory\n",
    "- Stack: Local variables and function calls\n",
    "\n",
    "Processes cannot directly access each other's memory. Communication between processes requires special mechanisms like pipes, shared memory segments, or message passing.\n",
    "## Thread Memory Model\n",
    "Threads within the same process share most memory, but maintain some private areas:\n",
    "\n",
    "### Shared Memory:\n",
    "\n",
    "- Code segment (program instructions)\n",
    "- Data segment (global variables)\n",
    "- Heap (dynamically allocated memory)\n",
    "- Open file descriptors\n",
    "- Signal handlers\n",
    "\n",
    "### Private Memory:\n",
    "\n",
    "- Stack: Each thread has its own stack for local variables and function calls\n",
    "- Registers: CPU register states are private to each thread\n",
    "- Program counter: Each thread tracks its own execution position\n",
    "\n",
    "## Key Memory Implications\n",
    "Isolation vs. Sharing:\n",
    "\n",
    "Processes provide strong isolation but require overhead for context switching\n",
    "Threads enable efficient data sharing but require careful synchronization\n",
    "\n",
    "Memory Overhead:\n",
    "\n",
    "Creating a new process duplicates the entire memory space\n",
    "Creating a new thread only adds a new stack (typically 1-8MB)\n",
    "\n",
    "Synchronization:\n",
    "\n",
    "Processes rarely need synchronization for memory access\n",
    "Threads require locks, mutexes, or other synchronization primitives to prevent race conditions when accessing shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xATmVKizsRHy"
   },
   "source": [
    "# GIL Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBa-DO97sbzb"
   },
   "source": [
    "## What is the Global Interpreter Lock (GIL)?\n",
    "The GIL is a mutex that protects Python objects, allowing only one thread to execute Python bytecode at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4376,
     "status": "ok",
     "timestamp": 1757644079110,
     "user": {
      "displayName": "Muhammad Yasin Nasir",
      "userId": "02641958579799766076"
     },
     "user_tz": -300
    },
    "id": "91eObx7_sV5C",
    "outputId": "684f66c9-9de8-4ea6-9c1e-99a3a0062b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 8 I/O tasks with 4 threads...\n",
      "\n",
      "[02:27:58] Thread-132373323032128 START task 0\n",
      "[02:27:58] Thread-132373257639488 START task 1\n",
      "[02:27:58] Thread-132373274424896 START task 2\n",
      "[02:27:58] Thread-132373266032192 START task 3\n",
      "[02:28:00] Thread-132373323032128 END task 0[02:28:00] Thread-132373257639488 END task 1\n",
      "[02:28:00] Thread-132373257639488 START task 4\n",
      "\n",
      "[02:28:00] Thread-132373323032128 START task 5\n",
      "[02:28:00] Thread-132373274424896 END task 2\n",
      "[02:28:00] Thread-132373274424896 START task 6\n",
      "[02:28:00] Thread-132373266032192 END task 3\n",
      "[02:28:00] Thread-132373266032192 START task 7\n",
      "[02:28:02] Thread-132373257639488 END task 4\n",
      "[02:28:02] Thread-132373266032192 END task 7\n",
      "[02:28:02] Thread-132373274424896 END task 6\n",
      "[02:28:02] Thread-132373323032128 END task 5\n",
      "\n",
      "Simulation complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def io_task(task_id, duration=1.0):\n",
    "    \"\"\"Simulated I/O task with logging.\"\"\"\n",
    "    thread_id = threading.get_ident()\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{start}] Thread-{thread_id} START task {task_id}\")\n",
    "\n",
    "    time.sleep(duration)  # Simulate blocking I/O (releases the GIL)\n",
    "\n",
    "    end = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{end}] Thread-{thread_id} END task {task_id}\")\n",
    "    return task_id\n",
    "\n",
    "def run_io_simulation(num_tasks=8, workers=4, duration=1.0):\n",
    "    print(f\"\\nRunning {num_tasks} I/O tasks with {workers} threads...\\n\")\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(io_task, i, duration) for i in range(num_tasks)]\n",
    "        for fut in as_completed(futures):\n",
    "            _ = fut.result()\n",
    "    print(\"\\nSimulation complete.\")\n",
    "\n",
    "# Run the demo\n",
    "run_io_simulation(num_tasks=8, workers=4, duration=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrczhTXlwKUp"
   },
   "source": [
    "it explains the GIL behavior clearly in an I/O-bound context:\n",
    "\n",
    "At 02:16:28, 4 threads (equal to the worker pool size) start tasks 0–3 simultaneously.\n",
    "\n",
    "Each one goes to sleep (time.sleep → simulating I/O), and importantly:\n",
    "\n",
    "time.sleep releases the GIL, so the interpreter can let other threads run freely.\n",
    "\n",
    "At 02:16:30, all 4 threads wake up almost together, finish tasks 0–3, and immediately pick up the next 4 tasks (4–7).\n",
    "\n",
    "At 02:16:32, all finish nearly together again.\n",
    "\n",
    "This shows:\n",
    "\n",
    "Threads are highly effective for I/O-bound workloads, since the GIL isn’t blocking while threads are sleeping or waiting for I/O.\n",
    "\n",
    "If this were CPU-bound work (tight Python loops), you’d see only one thread actually progressing at a time (the GIL would serialize them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1757644348099,
     "user": {
      "displayName": "Muhammad Yasin Nasir",
      "userId": "02641958579799766076"
     },
     "user_tz": -300
    },
    "id": "WPtjHQP1taQY",
    "outputId": "23a117e3-4576-4dd5-c4cf-48cdfc7a60fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 4 CPU-bound tasks with 4 threads...\n",
      "\n",
      "[02:32:31] Thread-132373323032128 START task 0\n",
      "[02:32:31] Thread-132373266032192 START task 1\n",
      "[02:32:31] Thread-132373274424896 START task 2\n",
      "[02:32:31] Thread-132373257639488 START task 3\n",
      "[02:32:32] Thread-132373266032192 END task 1\n",
      "[02:32:32] Thread-132373323032128 END task 0\n",
      "[02:32:32] Thread-132373257639488 END task 3\n",
      "[02:32:32] Thread-132373274424896 END task 2\n",
      "\n",
      "Simulation complete in 0.26 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import math\n",
    "\n",
    "def cpu_task(task_id, n=5_000_000):\n",
    "    \"\"\"Simulated CPU-bound task with logging.\"\"\"\n",
    "    thread_id = threading.get_ident()\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{start}] Thread-{thread_id} START task {task_id}\")\n",
    "\n",
    "    # CPU-intensive loop (stays under the GIL)\n",
    "    s = 0.0\n",
    "    for i in range(1, n):\n",
    "        s += math.sqrt(i) * math.sin(i)  # heavy but pure Python\n",
    "    result = s\n",
    "\n",
    "    end = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{end}] Thread-{thread_id} END task {task_id}\")\n",
    "    return task_id, result\n",
    "\n",
    "def run_cpu_simulation(num_tasks=4, workers=4, n=5_000_000):\n",
    "    print(f\"\\nRunning {num_tasks} CPU-bound tasks with {workers} threads...\\n\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(cpu_task, i, n) for i in range(num_tasks)]\n",
    "        for fut in as_completed(futures):\n",
    "            _ = fut.result()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"\\nSimulation complete in {t1 - t0:.2f} seconds.\")\n",
    "\n",
    "# Run the demo\n",
    "run_cpu_simulation(num_tasks=4, workers=4, n=5_000_00)  # adjust n upward for slower/clearer demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1506,
     "status": "ok",
     "timestamp": 1757643699051,
     "user": {
      "displayName": "Muhammad Yasin Nasir",
      "userId": "02641958579799766076"
     },
     "user_tz": -300
    },
    "id": "V9wrforOuHHQ",
    "outputId": "717f5949-9159-4f6c-ee0d-d7a661aa4c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 4 CPU-bound tasks with 4 threads...\n",
      "\n",
      "[02:21:41] Thread-132373257639488 START task 0\n",
      "[02:21:41] Thread-132373323032128 START task 1\n",
      "[02:21:41] Thread-132373266032192 START task 2\n",
      "[02:21:41] Thread-132373274424896 START task 3\n",
      "[02:21:41] Thread-132373257639488 working on task 0, iteration 500000\n",
      "[02:21:41] Thread-132373323032128 working on task 1, iteration 500000\n",
      "[02:21:42] Thread-132373266032192 working on task 2, iteration 500000\n",
      "[02:21:42] Thread-132373323032128 working on task 1, iteration 1000000\n",
      "[02:21:42] Thread-132373266032192 working on task 2, iteration 1000000\n",
      "[02:21:42] Thread-132373323032128 working on task 1, iteration 1500000\n",
      "[02:21:42] Thread-132373266032192 working on task 2, iteration 1500000\n",
      "[02:21:42] Thread-132373274424896 working on task 3, iteration 500000\n",
      "[02:21:42] Thread-132373266032192 working on task 2, iteration 2000000\n",
      "[02:21:42] Thread-132373266032192 END task 2\n",
      "[02:21:42] Thread-132373257639488 working on task 0, iteration 1000000\n",
      "[02:21:42] Thread-132373323032128 working on task 1, iteration 2000000\n",
      "[02:21:42] Thread-132373323032128 END task 1\n",
      "[02:21:42] Thread-132373274424896 working on task 3, iteration 1000000\n",
      "[02:21:42] Thread-132373257639488 working on task 0, iteration 1500000\n",
      "[02:21:42] Thread-132373274424896 working on task 3, iteration 1500000\n",
      "[02:21:43] Thread-132373257639488 working on task 0, iteration 2000000\n",
      "[02:21:43] Thread-132373257639488 END task 0\n",
      "[02:21:43] Thread-132373274424896 working on task 3, iteration 2000000\n",
      "[02:21:43] Thread-132373274424896 END task 3\n",
      "\n",
      "Simulation complete in 1.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import math\n",
    "\n",
    "def cpu_task(task_id, n=2_000_000, log_every=500_000):\n",
    "    \"\"\"CPU-bound task with per-iteration logging to show which thread is active.\"\"\"\n",
    "    thread_id = threading.get_ident()\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{start}] Thread-{thread_id} START task {task_id}\")\n",
    "\n",
    "    s = 0.0\n",
    "    for i in range(1, n + 1):\n",
    "        s += math.sqrt(i) * math.sin(i)\n",
    "\n",
    "        # Log occasionally to avoid flooding the output\n",
    "        if i % log_every == 0:\n",
    "            ts = time.strftime(\"%H:%M:%S\")\n",
    "            print(f\"[{ts}] Thread-{thread_id} working on task {task_id}, iteration {i}\")\n",
    "\n",
    "    end = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{end}] Thread-{thread_id} END task {task_id}\")\n",
    "    return task_id, s\n",
    "\n",
    "def run_cpu_simulation(num_tasks=4, workers=4, n=2_000_000):\n",
    "    print(f\"\\nRunning {num_tasks} CPU-bound tasks with {workers} threads...\\n\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(cpu_task, i, n) for i in range(num_tasks)]\n",
    "        for fut in as_completed(futures):\n",
    "            _ = fut.result()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"\\nSimulation complete in {t1 - t0:.2f} seconds.\")\n",
    "\n",
    "# Run the demo\n",
    "run_cpu_simulation(num_tasks=4, workers=4, n=2_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNBRpHyZumxG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPsoSCw60qyEbTuqhQYOHyf",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12 (ray)",
   "language": "python",
   "name": "ray312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
