{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing in Python: Unlocking True Parallelism\n",
    "https://docs.python.org/3/library/multiprocessing.html\n",
    "\n",
    "These notes cover how to use Python's `multiprocessing` module to perform parallel processing, effectively bypassing the Global Interpreter Lock (GIL) to achieve significant performance gains for CPU-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Multiprocessing? The Global Interpreter Lock (GIL)\n",
    "\n",
    "At the heart of CPython is the **Global Interpreter Lock (GIL)**, a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at the same time within a single process.\n",
    "\n",
    "- **Multithreading (`threading` module):** Excellent for I/O-bound tasks (e.g., network requests, file operations). While one thread waits for I/O, the GIL is released, allowing another thread to run. However, for CPU-bound tasks (e.g., complex calculations, data processing), threads don't offer true parallelism; they just take turns executing on a single CPU core.\n",
    "- **Multiprocessing (`multiprocessing` module):** The solution for CPU-bound tasks. This module creates separate processes, each with its own Python interpreter and memory space. Since each process has its own GIL, they can run in parallel on different CPU cores, achieving true concurrency. ðŸš€\n",
    "\n",
    "The key takeaway is:\n",
    "- **I/O-bound task?** Use **multithreading**.\n",
    "- **CPU-bound task?** Use **multiprocessing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The `multiprocessing` Module: Core Concepts\n",
    "\n",
    "The `multiprocessing` module's API is intentionally similar to the `threading` module's, making it easy to transition.\n",
    "\n",
    "#### Creating and Running Processes\n",
    "\n",
    "The fundamental object is the `multiprocessing.Process` class.\n",
    "\n",
    "**Key Methods:**\n",
    "- `p = Process(target=func, args=(arg1, arg2))`: Instantiates a process. `target` is the callable object (function) to be invoked by the `run()` method. `args` is the tuple of arguments for the target.\n",
    "- `p.start()`: Starts the process's activity. It arranges for the objectâ€™s `run()` method to be invoked in a separate process.\n",
    "- `p.join()`: Blocks the main program until the process whose `join()` method is called terminates. This is crucial for ensuring processes complete before the main script exits.\n",
    "- `p.is_alive()`: Returns `True` if the process is still running.\n",
    "- `os.getpid()`: Returns the current process ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Basic Process Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main process ID: 18228\n",
      "Main program continues to run while worker is executing...\n",
      "Worker process has finished. Main program exiting.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "def worker_function(name):\n",
    "    \"\"\"A simple function that a process will execute.\"\"\"\n",
    "    print(f\"Worker '{name}' started. Process ID: {os.getpid()}\")\n",
    "    time.sleep(2)\n",
    "    print(f\"Worker '{name}' finished.\")\n",
    "\n",
    "# On Windows, this guard is essential!\n",
    "# In a Jupyter cell, this check doesn't behave the same way as a .py script.\n",
    "# For demonstration, we'll run it directly, but remember to use the guard in standalone scripts.\n",
    "\n",
    "print(f\"Main process ID: {os.getpid()}\")\n",
    "\n",
    "# Create a process\n",
    "p = multiprocessing.Process(target=worker_function, args=('Alice',))\n",
    "\n",
    "# Start the process\n",
    "p.start()\n",
    "\n",
    "print(\"Main program continues to run while worker is executing...\")\n",
    "\n",
    "# Wait for the process to complete\n",
    "p.join()\n",
    "\n",
    "print(\"Worker process has finished. Main program exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `if __name__ == '__main__':` Guard\n",
    "\n",
    "On Windows (and macOS), the `multiprocessing` module uses the **'spawn'** start method by default. This means a new child Python interpreter is started for each new process. This child interpreter re-imports the script. Without the `if __name__ == '__main__':` guard, the process creation code would run again in the child process, leading to an infinite loop of process creation and a `RuntimeError`.\n",
    "\n",
    "**Therefore, on Windows, all multiprocessing code in `.py` scripts must be placed inside this block.** Jupyter notebooks handle this context differently, so you may not always see an error, but it is a critical best practice when writing standalone applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inter-Process Communication (IPC) and Synchronization\n",
    "\n",
    "Since processes have separate memory spaces, they cannot directly share data like threads. We need explicit IPC mechanisms.\n",
    "\n",
    "#### `multiprocessing.Queue`\n",
    "\n",
    "A **process-safe and thread-safe** FIFO queue. It's the most common and robust way to pass messages between processes. Data is pickled before being sent and unpickled upon receipt.\n",
    "\n",
    "- `q.put(obj)`: Puts an object onto the queue.\n",
    "- `q.get()`: Removes and returns an object from the queue. Blocks until an item is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Using a Queue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks finished.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def producer(q):\n",
    "    \"\"\"Puts items into the queue.\"\"\"\n",
    "    for i in range(5):\n",
    "        item = f\"Item {i}\"\n",
    "        q.put(item)\n",
    "        print(f\"Produced: {item}\")\n",
    "        time.sleep(0.5)\n",
    "    q.put(None) # Sentinel value to signal completion\n",
    "\n",
    "def consumer(q):\n",
    "    \"\"\"Gets items from the queue.\"\"\"\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        if item is None: # Check for sentinel\n",
    "            break\n",
    "        print(f\"Consumed: {item}\")\n",
    "\n",
    "queue = multiprocessing.Queue()\n",
    "\n",
    "p1 = multiprocessing.Process(target=producer, args=(queue,))\n",
    "p2 = multiprocessing.Process(target=consumer, args=(queue,))\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "print(\"All tasks finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `multiprocessing.Pipe`\n",
    "\n",
    "A `Pipe()` returns a pair of connection objects connected by a pipe which by default is duplex (two-way). Pipes are generally faster than Queues but are limited to communication between two processes.\n",
    "\n",
    "- `parent_conn, child_conn = Pipe()`: Creates the pipe.\n",
    "- `conn.send(obj)`: Sends an object.\n",
    "- `conn.recv()`: Receives an object.\n",
    "\n",
    "#### Shared Memory: `Value` and `Array`\n",
    "\n",
    "For sharing simple data types, you can use `Value` and `Array`, which provide shared memory mapped objects. These should always be protected with a `Lock` to prevent race conditions.\n",
    "\n",
    "- `num = multiprocessing.Value('d', 0.0)`: A shared double, initialized to 0.0.\n",
    "- `arr = multiprocessing.Array('i', range(10))`: A shared array of integers.\n",
    "- `lock = multiprocessing.Lock()`: Creates a lock object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Multiprocessing Pipe Example ---\n",
      "[Pipe Parent, PID: 18228] Sending message: 'hello from parent'\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Example 1: Using multiprocessing.Pipe for Two-Way Communication ---\n",
    "\n",
    "def pipe_child_process(conn):\n",
    "    \"\"\"\n",
    "    This function runs in the child process.\n",
    "    It receives data from the parent, processes it, and sends a response back.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Pipe Child, PID: {os.getpid()}] Waiting to receive a message...\")\n",
    "    \n",
    "    # conn.recv() will block until a message is available on the pipe\n",
    "    message_from_parent = conn.recv()\n",
    "    print(f\"[Pipe Child, PID: {os.getpid()}] Received: '{message_from_parent}'\")\n",
    "    \n",
    "    # Process the data (e.g., make it uppercase)\n",
    "    processed_message = message_from_parent.upper()\n",
    "    \n",
    "    # Send the processed data back to the parent\n",
    "    conn.send(processed_message)\n",
    "    print(f\"[Pipe Child, PID: {os.getpid()}] Sent response: '{processed_message}'\")\n",
    "    \n",
    "    # Close the connection end in the child process\n",
    "    conn.close()\n",
    "\n",
    "def run_pipe_example():\n",
    "    \"\"\"\n",
    "    Demonstrates two-way communication between a parent and child process\n",
    "    using a Pipe.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Multiprocessing Pipe Example ---\")\n",
    "    \n",
    "    # 1. Create a Pipe. It returns two connection objects.\n",
    "    # By default, the pipe is duplex (two-way).\n",
    "    parent_conn, child_conn = multiprocessing.Pipe()\n",
    "\n",
    "    # 2. Create a Process, giving it one end of the pipe (child_conn)\n",
    "    p = multiprocessing.Process(target=pipe_child_process, args=(child_conn,))\n",
    "    p.start()\n",
    "\n",
    "    print(f\"[Pipe Parent, PID: {os.getpid()}] Sending message: 'hello from parent'\")\n",
    "    # 3. Parent sends an object through its end of the pipe (parent_conn)\n",
    "    parent_conn.send(\"hello from parent\")\n",
    "\n",
    "    # 4. Parent waits to receive the response from the child\n",
    "    response = parent_conn.recv()\n",
    "    print(f\"[Pipe Parent, PID: {os.getpid()}] Received response: '{response}'\")\n",
    "\n",
    "    # 5. Wait for the child process to finish\n",
    "    p.join()\n",
    "\n",
    "    # Close the parent's end of the pipe\n",
    "    parent_conn.close()\n",
    "    print(\"\\n--- Pipe Example Finished ---\")\n",
    "\n",
    "\n",
    "# --- Example 2: Using Shared Memory (Value and Array) with a Lock ---\n",
    "\n",
    "def shared_memory_worker(shared_value, shared_array, lock):\n",
    "    \"\"\"\n",
    "    This function is executed by multiple processes.\n",
    "    It uses a lock to safely modify a shared counter (Value) and a shared Array.\n",
    "    \"\"\"\n",
    "    process_id = os.getpid()\n",
    "    for _ in range(5):\n",
    "        # The 'with' statement automatically acquires and releases the lock.\n",
    "        # This prevents race conditions.\n",
    "        with lock:\n",
    "            # Safely increment the shared value\n",
    "            shared_value.value += 1\n",
    "            \n",
    "            # Safely modify the shared array\n",
    "            # Here, we increment each element of the array\n",
    "            for i in range(len(shared_array)):\n",
    "                shared_array[i] += 1\n",
    "        \n",
    "        print(f\"[Shared Mem Worker, PID: {process_id}] Incremented. Current value: {shared_value.value}\")\n",
    "        time.sleep(0.01) # Small delay to make execution order more apparent\n",
    "\n",
    "def run_shared_memory_example():\n",
    "    \"\"\"\n",
    "    Demonstrates sharing state between processes using Value and Array,\n",
    "    synchronized with a Lock to prevent race conditions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- Starting Shared Memory (Value, Array, Lock) Example ---\")\n",
    "    \n",
    "    # 1. Create a Lock to synchronize access to shared resources\n",
    "    lock = multiprocessing.Lock()\n",
    "    \n",
    "    # 2. Create shared memory objects\n",
    "    # 'd' is a type code for a double-precision float.\n",
    "    # 'i' is a type code for a signed integer.\n",
    "    shared_counter = multiprocessing.Value('i', 0)\n",
    "    initial_array = [10, 20, 30, 40]\n",
    "    shared_array = multiprocessing.Array('i', initial_array)\n",
    "    \n",
    "    print(f\"Initial shared counter: {shared_counter.value}\")\n",
    "    # Slicing is needed to print the array's contents nicely\n",
    "    print(f\"Initial shared array: {shared_array[:]}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 3. Create a pool of worker processes\n",
    "    num_processes = 4\n",
    "    processes = []\n",
    "    for _ in range(num_processes):\n",
    "        p = multiprocessing.Process(\n",
    "            target=shared_memory_worker, \n",
    "            args=(shared_counter, shared_array, lock)\n",
    "        )\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # 4. Wait for all processes to complete their execution\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        \n",
    "    print(\"-\" * 20)\n",
    "    print(\"Final Results:\")\n",
    "    print(f\"Final shared counter: {shared_counter.value}\")\n",
    "    print(f\"Final shared array: {shared_array[:]}\")\n",
    "    print(\"\\n--- Shared Memory Example Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # On Windows, all multiprocessing code must be inside this block\n",
    "    # to prevent issues with child processes re-importing and re-executing the script.\n",
    "    \n",
    "    # Run the first example for Pipe\n",
    "    run_pipe_example()\n",
    "    \n",
    "    # Run the second example for shared memory\n",
    "    run_shared_memory_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Shared Counter with a Lock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final counter value: 0\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def increment(shared_value, lock):\n",
    "    for _ in range(10000):\n",
    "        with lock:\n",
    "            shared_value.value += 1\n",
    "\n",
    "shared_counter = multiprocessing.Value('i', 0)\n",
    "lock = multiprocessing.Lock()\n",
    "\n",
    "processes = [multiprocessing.Process(target=increment, args=(shared_counter, lock)) for _ in range(4)]\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(f\"Final counter value: {shared_counter.value}\") # Should be 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Managing Pools of Workers: `multiprocessing.Pool`\n",
    "\n",
    "Creating and managing individual processes can be tedious. The `Pool` object offers a convenient way to parallelize the execution of a function across multiple input values, distributing the input data across a fixed number of processes (a \"pool\").\n",
    "\n",
    "`pool = multiprocessing.Pool(processes=os.cpu_count())`\n",
    "\n",
    "#### Key Pool Methods\n",
    "\n",
    "- **Blocking (Synchronous) Methods:** These wait for all results to be completed.\n",
    "    - `pool.map(func, iterable)`: Applies `func` to each item in `iterable`. It chunks the iterable and distributes it to the worker processes. Returns a list of results. It's like a parallel version of the built-in `map()`.\n",
    "    - `pool.apply(func, args)`: Calls `func` with arguments `args`. This is for a single task execution and blocks until the function is complete.\n",
    "\n",
    "- **Non-Blocking (Asynchronous) Methods:** These return immediately, allowing the main program to do other work. The results are obtained later.\n",
    "    - `pool.map_async(func, iterable)`: The async version of `map`. Returns an `AsyncResult` object.\n",
    "    - `pool.apply_async(func, args)`: The async version of `apply`. Returns an `AsyncResult` object.\n",
    "\n",
    "To get the result from an `AsyncResult` object (`res`), you call `res.get()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: CPU-Bound Task with `Pool.map`**\n",
    "\n",
    "Let's simulate a heavy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"A CPU-intensive function.\"\"\"\n",
    "    # Simulate work\n",
    "    # time.sleep(0.01)\n",
    "    return x * x\n",
    "\n",
    "# Note: In a script, this would be inside the if __name__ == '__main__': block\n",
    "\n",
    "# Use all available CPU cores\n",
    "pool = multiprocessing.Pool(processes=os.cpu_count())\n",
    "\n",
    "inputs = range(20)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# map blocks until all results are ready\n",
    "results = pool.map(square, inputs)\n",
    "\n",
    "# Always close and join a pool\n",
    "pool.close() # Prevents any more tasks from being submitted\n",
    "pool.join()  # Waits for the worker processes to exit\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Results: {results}\")\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Topics and Best Practices\n",
    "\n",
    "- **Choosing Chunksize in `pool.map`:** For very long iterables, specifying a `chunksize` can significantly improve performance by reducing the overhead of sending small chunks of data to worker processes. A good starting point is `chunksize = len(iterable) // (len(pool) * 4)`.\n",
    "\n",
    "- **Process vs. `Pool`:** Use `Process` for long-running, heterogeneous tasks (e.g., a producer and a consumer). Use `Pool` for homogeneous, embarrassingly parallel tasks where you need to apply the same function to many different pieces of data.\n",
    "\n",
    "- **Overhead:** Creating processes is not free. There's a performance cost for process creation, data pickling/unpickling (serialization), and IPC. Multiprocessing is only beneficial if the task's computation time is significantly greater than this overhead.\n",
    "\n",
    "- **Daemon Processes:** A process can be flagged as a daemon process (`p.daemon = True`). Daemon processes are terminated automatically when the main program exits. They are useful for background tasks but cannot create new processes themselves.\n",
    "\n",
    "- **Exception Handling in Pools:** If a worker in a `Pool` raises an exception, that exception will be re-raised in the main process when you try to retrieve the result with `.get()`. It's crucial to wrap your result-retrieval calls in `try...except` blocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
